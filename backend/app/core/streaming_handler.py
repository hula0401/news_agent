"""Streaming voice handler for real-time audio processing."""
import asyncio
import base64
import hashlib
from typing import AsyncGenerator, Optional, Dict, Any
from datetime import datetime

try:
    import edge_tts
except ImportError:
    edge_tts = None

try:
    from funasr import AutoModel
except ImportError:
    AutoModel = None

try:
    from .hf_space_asr import get_hf_space_asr
    HF_SPACE_AVAILABLE = True
except ImportError:
    HF_SPACE_AVAILABLE = False

try:
    from .audio_validator import get_audio_validator
    AUDIO_VALIDATOR_AVAILABLE = True
except ImportError:
    AUDIO_VALIDATOR_AVAILABLE = False


class StreamingVoiceHandler:
    """Handle streaming voice input/output."""

    def __init__(self):
        self.audio_buffers = {}  # session_id -> buffer
        self.transcription_cache = {}  # session_id -> partial text
        self.sensevoice_model = None
        self._model_loaded = False
        self.hf_space_asr = None
        self._hf_space_enabled = True  # Prefer HF Space by default
        self.audio_validator = None

        # Get configuration
        from ..config import get_settings
        self.settings = get_settings()
        self._use_local_asr = self.settings.use_local_asr

        # Initialize audio validator with config settings
        if AUDIO_VALIDATOR_AVAILABLE:
            self.audio_validator = get_audio_validator(
                energy_threshold=self.settings.vad_energy_threshold,
                vad_mode=self.settings.vad_aggressiveness,
                enable_webrtc_vad=True,
                speech_ratio_threshold=self.settings.vad_speech_ratio_threshold
            )
    
    async def load_sensevoice_model(self, model_path: str = "models/SenseVoiceSmall"):
        """Load SenseVoice model for ASR (same as src implementation)."""
        if AutoModel is None:
            print("âš ï¸ FunASR not available, using fallback ASR")
            return False
            
        try:
            print(f"ğŸ”„ Loading SenseVoice model: {model_path}")
            self.sensevoice_model = AutoModel(
                model=model_path,
                trust_remote_code=True
            )
            self._model_loaded = True
            print("âœ… SenseVoice model loaded successfully")
            return True
        except Exception as e:
            print(f"âŒ Failed to load SenseVoice model: {e}")
            return False
    
    async def stream_tts_audio(
        self,
        text: str,
        voice: str = "en-US-AriaNeural",
        rate: str = "+0%",
        chunk_size: int = 4096
    ) -> AsyncGenerator[bytes, None]:
        """
        Stream TTS audio in chunks using Edge-TTS.

        Args:
            text: Text to convert to speech
            voice: Voice to use
            rate: Speech rate adjustment
            chunk_size: Size of each chunk in bytes

        Yields:
            Audio chunks as bytes
        """
        if edge_tts is None:
            # Fallback: return empty chunks if edge-tts not available
            print("âš ï¸ edge-tts not available, skipping TTS streaming")
            return

        # Validate text input
        if not text or not text.strip():
            print("âš ï¸ Empty text provided to TTS, skipping")
            return

        try:
            # Create communicate object with SSL context handling
            print(f"ğŸ”Š Starting TTS for text (length: {len(text)}): {text[:50]}...")
            communicate = edge_tts.Communicate(text, voice, rate=rate)

            buffer = bytearray()
            chunk_count = 0
            async for chunk in communicate.stream():
                if chunk["type"] == "audio":
                    buffer.extend(chunk["data"])

                    # Yield chunks of specified size
                    while len(buffer) >= chunk_size:
                        yield bytes(buffer[:chunk_size])
                        chunk_count += 1
                        buffer = buffer[chunk_size:]

            # Yield remaining data
            if buffer:
                yield bytes(buffer)
                chunk_count += 1

            print(f"âœ… TTS completed: {chunk_count} chunks generated")

        except Exception as e:
            error_msg = str(e)

            # Check for specific edge-tts errors
            if "No audio was received" in error_msg:
                print(f"âš ï¸ Edge-TTS error: No audio received")
                print(f"   Text: {text[:100]}")
                print(f"   Voice: {voice}")
                print(f"   Rate: {rate}")
                print(f"   Possible causes:")
                print(f"   1. Network connectivity issues (check firewall/proxy)")
                print(f"   2. Microsoft Edge TTS service temporarily unavailable")
                print(f"   3. Invalid voice name (use edge-tts --list-voices to verify)")
                print(f"   4. Text contains unsupported characters")
            # Check if it's an SSL certificate error
            elif "SSL" in error_msg or "certificate" in error_msg.lower():
                print(f"âš ï¸ TTS SSL certificate error: {error_msg}")
                print(f"   This is a known issue with edge-tts and api.msedgeservices.com")
                print(f"   Possible solutions:")
                print(f"   1. Update certifi: uv pip install --upgrade certifi")
                print(f"   2. Update edge-tts: uv pip install --upgrade edge-tts")
                print(f"   3. Check system date/time settings")
                print(f"   4. Use alternative TTS service (see docs)")
            else:
                print(f"âŒ Error streaming TTS: {e}")

            raise
    
    async def buffer_audio_chunk(
        self, 
        session_id: str, 
        audio_chunk: bytes,
        is_final: bool = False
    ) -> Optional[bytes]:
        """
        Buffer incoming audio chunks and return full buffer when ready.
        
        Args:
            session_id: Session identifier
            audio_chunk: Audio data chunk
            is_final: Whether this is the final chunk
            
        Returns:
            Full audio buffer if ready for processing, None otherwise
        """
        if session_id not in self.audio_buffers:
            self.audio_buffers[session_id] = bytearray()
        
        self.audio_buffers[session_id].extend(audio_chunk)
        
        # Process if buffer is large enough (1 second at 16kHz, 16-bit)
        # or if final chunk
        buffer_size_threshold = 32000  # ~1 second of audio
        
        if len(self.audio_buffers[session_id]) >= buffer_size_threshold or is_final:
            full_buffer = bytes(self.audio_buffers[session_id])
            self.audio_buffers[session_id].clear()
            return full_buffer
        
        return None
    
    async def transcribe_chunk(
        self,
        audio_data: bytes,
        format: str = "wav",
        sample_rate: int = 16000,
        validate_audio: bool = True
    ) -> str:
        """
        Transcribe audio chunk with support for compressed formats.

        Uses HuggingFace Space as primary ASR, falls back to local model.

        Args:
            audio_data: Raw audio bytes (may be compressed)
            format: Audio format (wav, opus, webm, mp3, etc.)
            sample_rate: Sample rate in Hz
            validate_audio: Enable audio validation (energy + WebRTC VAD)

        Returns:
            Transcribed text
        """
        # Validate audio quality before processing
        if validate_audio and self.audio_validator and format in ["wav", "pcm"]:
            is_valid, validation_info = self.audio_validator.validate_audio(
                audio_data,
                sample_rate=sample_rate,
                format=format
            )

            energy = validation_info.get("energy", 0)
            speech_ratio = validation_info.get("webrtc_speech_ratio", 0)

            if not is_valid:
                reason = validation_info.get("reason", "unknown")

                # More informative logging for validation failures
                if reason == "insufficient_energy":
                    print(f"ğŸš« VAD REJECTED: No speech detected (energy={energy:.1f}, threshold={self.audio_validator.energy_threshold})")
                elif reason == "no_speech_detected":
                    print(f"ğŸš« VAD REJECTED: WebRTC rejected audio (speech_ratio={speech_ratio:.2f}, threshold={self.audio_validator.speech_ratio_threshold})")
                else:
                    print(f"ğŸš« VAD REJECTED: Audio validation failed: {reason} (energy={energy:.1f})")

                # Don't raise error, just return empty string to indicate no transcription
                return ""
            else:
                # Log successful validation
                print(f"âœ… VAD ACCEPTED: Audio validated (energy={energy:.1f}, speech_ratio={speech_ratio:.2f}, size={len(audio_data)} bytes)")


        # Try HuggingFace Space first (preferred for production)
        if self._hf_space_enabled and HF_SPACE_AVAILABLE:
            try:
                if self.hf_space_asr is None:
                    self.hf_space_asr = get_hf_space_asr()

                # Convert to WAV if needed
                wav_data = await self._convert_to_wav(audio_data, format, sample_rate)

                print(f"ğŸŒ Using HF Space ASR: {len(audio_data)} bytes ({format})")

                # Transcribe using HF Space
                transcription = await self.hf_space_asr.transcribe_audio_bytes(
                    wav_data,
                    sample_rate=sample_rate,
                    format="wav"
                )

                print(f"âœ“ HF Space transcribed: '{transcription}'")
                return transcription

            except Exception as e:
                print(f"âš ï¸ HF Space ASR failed: {e}")

                # Only fallback if local ASR is enabled
                if not self._use_local_asr:
                    print(f"âŒ Local ASR disabled (USE_LOCAL_ASR=false), no fallback available")
                    raise RuntimeError("HF Space ASR failed and local ASR is disabled")

                print(f"   Falling back to local model...")

        # Fallback to local model (only if enabled)
        if not self._use_local_asr:
            print(f"âŒ Local ASR disabled (USE_LOCAL_ASR=false)")
            raise RuntimeError("Speech recognition unavailable. HF Space failed and local ASR is disabled.")

        if not self._model_loaded or self.sensevoice_model is None:
            print(f"âŒ No ASR available (HF Space failed, local model not loaded)")
            raise RuntimeError("Speech recognition unavailable. Please check configuration.")

        try:
            # Convert compressed audio to WAV if needed
            wav_data = await self._convert_to_wav(audio_data, format, sample_rate)

            # Save audio to temporary file
            import tempfile
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmpfile:
                tmpfile.write(wav_data)
                audio_file = tmpfile.name

            print(f"ğŸ¤ Using local model: {len(audio_data)} bytes ({format}) -> {len(wav_data)} bytes (wav)")

            # Transcribe with local SenseVoice
            result = self.sensevoice_model.generate(
                input=audio_file,
                cache={},
                language="auto",
                use_itn=False,
            )

            # Clean up temp file
            import os
            os.unlink(audio_file)

            if result and len(result) > 0 and 'text' in result[0]:
                # Extract text (remove language tags)
                text = result[0]['text'].split(">")[-1].strip()
                print(f"âœ“ Local model transcribed: '{text}'")
                return text
            else:
                raise RuntimeError("Transcription model returned empty result")

        except Exception as e:
            print(f"âŒ Transcription error: {e}")
            raise
    
    async def _convert_to_wav(self, audio_data: bytes, format: str, sample_rate: int = 16000) -> bytes:
        """
        Convert compressed audio to WAV format using FFmpeg.
        
        Args:
            audio_data: Raw audio bytes
            format: Source format (opus, webm, mp3, etc.)
            sample_rate: Target sample rate
            
        Returns:
            WAV audio bytes
        """
        import tempfile
        import subprocess
        import os
        
        # If already WAV, return as-is
        if format.lower() == "wav":
            return audio_data
        
        input_path = None
        output_path = None

        try:
            # Create temporary files
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{format}") as input_file:
                input_file.write(audio_data)
                input_path = input_file.name

            print(f"ğŸ“ Created temp input file: {input_path} ({len(audio_data)} bytes)")

            # DEBUG: Save a copy for inspection
            debug_path = f"/tmp/debug_audio_{os.path.basename(input_path)}"
            import shutil
            shutil.copy(input_path, debug_path)
            print(f"ğŸ› DEBUG: Saved copy to {debug_path}")

            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as output_file:
                output_path = output_file.name

            print(f"ğŸ“ Created temp output file: {output_path}")

            # FFmpeg command to convert to WAV with better error handling
            ffmpeg_cmd = [
                'ffmpeg',
                '-v', 'error',  # Only show errors
                '-i', input_path,
                '-ar', str(sample_rate),
                '-ac', '1',  # Mono
                '-f', 'wav',
                '-acodec', 'pcm_s16le',  # 16-bit PCM
                '-y',  # Overwrite output
                output_path
            ]

            print(f"ğŸ”§ Running FFmpeg: {' '.join(ffmpeg_cmd)}")

            # Run FFmpeg conversion
            result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True, check=True)
            
            # Read converted WAV data
            with open(output_path, 'rb') as f:
                wav_data = f.read()
            
            # Clean up temp files
            os.unlink(input_path)
            os.unlink(output_path)
            
            print(f"âœ… Converted {format} to WAV: {len(audio_data)} -> {len(wav_data)} bytes")
            return wav_data
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ FFmpeg conversion failed: {e.stderr}")
            print(f"   Input file: {input_path} exists={os.path.exists(input_path) if input_path else 'N/A'}")
            print(f"   Output file: {output_path} exists={os.path.exists(output_path) if output_path else 'N/A'}")
            # Re-raise error instead of returning bad data
            raise RuntimeError(f"FFmpeg conversion failed: {e.stderr}")
        except Exception as e:
            print(f"âŒ Audio conversion error: {e}")
            raise RuntimeError(f"Audio conversion error: {e}")
        finally:
            # Ensure temp files are cleaned up
            try:
                if input_path and os.path.exists(input_path):
                    os.unlink(input_path)
                if output_path and os.path.exists(output_path):
                    os.unlink(output_path)
            except Exception as cleanup_error:
                print(f"âš ï¸ Failed to cleanup temp files: {cleanup_error}")
    
    async def process_voice_command(
        self,
        session_id: str,
        audio_chunk: bytes,
        format: str = "webm"
    ) -> Dict[str, Any]:
        """
        Process incoming audio chunk through ASR, LLM, and TTS.
        This method orchestrates the full voice pipeline.
        """
        from ..core.agent_wrapper import get_agent # Lazy import to avoid circular dependency
        agent = await get_agent()

        try:
            # 1. ASR: Transcribe audio chunk
            transcription = await self.transcribe_chunk(audio_chunk, format)

            if not transcription:
                return {"success": False, "error": "No transcription"}

            # 2. LLM: Get agent response
            user_id = "anonymous" # TODO: Get actual user_id from session
            response_result = await agent.process_voice_command(transcription, user_id, session_id)
            response_text = response_result.get("response_text", "I'm sorry, I couldn't process that.")

            return {
                "success": True,
                "transcription": transcription,
                "response": response_text,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            print(f"âŒ Error in full voice pipeline: {e}")
            return {"success": False, "error": str(e)}

    async def process_voice_command_streaming(
        self,
        session_id: str,
        audio_chunk: bytes,
        format: str = "webm"
    ):
        """
        Process voice command with streaming LLM response and concurrent TTS.
        This yields both transcription and audio chunks as they become available.

        Yields:
            Dict with either:
            - {"type": "transcription", "text": str} - The transcribed user input
            - {"type": "text_chunk", "text": str} - A chunk of LLM response text
            - {"type": "audio_chunk", "data": bytes} - A chunk of TTS audio
            - {"type": "error", "message": str} - An error message
            - {"type": "complete"} - Indicates streaming is complete
        """
        from ..core.agent_wrapper import get_agent
        agent = await get_agent()

        try:
            # 1. ASR: Transcribe audio chunk
            print(f"ğŸ¤ Starting ASR for session {session_id}")
            transcription = await self.transcribe_chunk(audio_chunk, format)

            if not transcription:
                yield {"type": "error", "message": "No transcription"}
                return

            # Yield transcription
            print(f"âœ… Transcribed: {transcription}")
            yield {"type": "transcription", "text": transcription}

            # 2. Stream LLM response and do TTS concurrently
            user_id = "anonymous"  # TODO: Get actual user_id from session

            # Buffer for accumulating text for TTS
            text_buffer = ""
            sentence_endings = [".", "!", "?", "\n"]

            print(f"ğŸ¤– Starting streaming LLM response for: {transcription}")

            async for text_chunk in agent.stream_voice_response(transcription, user_id, session_id):
                if not text_chunk:
                    continue

                # Yield the text chunk for display
                yield {"type": "text_chunk", "text": text_chunk}

                # Accumulate text for TTS
                text_buffer += text_chunk

                # Check if we have a complete sentence to speak
                # This enables faster response by starting TTS before full response completes
                should_speak = False
                for ending in sentence_endings:
                    if ending in text_buffer:
                        should_speak = True
                        break

                # Or if buffer is long enough (avoid very long waits)
                if len(text_buffer) > 100:
                    should_speak = True

                if should_speak and text_buffer.strip():
                    # Stream TTS for the accumulated text
                    print(f"ğŸ”Š Starting TTS for: {text_buffer[:50]}...")
                    async for audio_chunk_data in self.stream_tts_audio(text_buffer.strip()):
                        yield {"type": "audio_chunk", "data": audio_chunk_data}

                    # Clear buffer after speaking
                    text_buffer = ""

            # Speak any remaining text
            if text_buffer.strip():
                print(f"ğŸ”Š Starting TTS for remaining text: {text_buffer[:50]}...")
                async for audio_chunk_data in self.stream_tts_audio(text_buffer.strip()):
                    yield {"type": "audio_chunk", "data": audio_chunk_data}

            # Signal completion
            yield {"type": "complete"}
            print(f"âœ… Completed streaming for session {session_id}")

        except Exception as e:
            print(f"âŒ Error in streaming voice pipeline: {e}")
            import traceback
            traceback.print_exc()
            yield {"type": "error", "message": str(e)}
    
    def clear_session_buffer(self, session_id: str):
        """Clear audio buffer for a session."""
        if session_id in self.audio_buffers:
            del self.audio_buffers[session_id]
        if session_id in self.transcription_cache:
            del self.transcription_cache[session_id]
    
    async def process_partial_transcription(
        self,
        session_id: str,
        partial_text: str,
        is_final: bool = False
    ) -> str:
        """
        Process partial transcription results.
        
        Args:
            session_id: Session identifier
            partial_text: Partial transcription
            is_final: Whether this is final
            
        Returns:
            Accumulated transcription text
        """
        if session_id not in self.transcription_cache:
            self.transcription_cache[session_id] = ""
        
        if is_final:
            # Return final text and clear cache
            final_text = self.transcription_cache[session_id] + " " + partial_text
            del self.transcription_cache[session_id]
            return final_text.strip()
        else:
            # Accumulate partial results
            self.transcription_cache[session_id] += " " + partial_text
            return self.transcription_cache[session_id].strip()
    
    def get_stream_stats(self) -> dict:
        """Get statistics about active streams."""
        return {
            "active_buffers": len(self.audio_buffers),
            "total_buffer_size": sum(len(buf) for buf in self.audio_buffers.values()),
            "active_transcriptions": len(self.transcription_cache)
        }


# Global streaming handler instance
streaming_handler = StreamingVoiceHandler()


def get_streaming_handler() -> StreamingVoiceHandler:
    """Get streaming handler instance."""
    return streaming_handler

