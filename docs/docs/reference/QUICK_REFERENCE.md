# Quick Reference Card

## ğŸš€ One-Line Commands

```bash
# Demo streaming
uv run python -m src.main_streaming --demo

# Run all tests
uv run python tests/run_all_tests.py --quick

# Start backend server
make run-server

# Fix TTS SSL errors
uv pip install --upgrade certifi edge-tts
```

---

## ğŸ“Š TTS Streaming Status

| Component | Generation | Playback | Overall |
|-----------|-----------|----------|---------|
| **backend/** | âœ… YES (4KB chunks) | âœ… YES (progressive) | âœ… FULL |
| **src/** | âœ… YES (4KB chunks) | âŒ NO (complete file) | âš ï¸ PARTIAL |

**Performance**:
- backend/: ~700ms to first audio (79% faster) âš¡
- src/: ~2950ms to first audio

---

## ğŸ” LLM Streaming Status

**Current**: âš ï¸ **Simulated** (breaks complete response into chunks)

**Why**: LangChain AgentExecutor limitation with tools

**Options**:
1. `get_response_stream()` - Simulated (keeps agent tools) âœ…
2. `get_response_stream_direct()` - True streaming (no tools) âš ï¸

**Future**: Migrate to LangGraph for true streaming + tools

---

## ğŸ“ Key Files

### Implementation:
- `src/agent.py:314-420` - LLM streaming
- `src/voice_output.py:254-358` - TTS streaming
- `src/main_streaming.py` - Demo app
- `backend/app/core/streaming_handler.py:406-494` - Pipeline
- `backend/app/core/websocket_manager.py:611-787` - WebSocket

### Tests:
- `tests/src/test_streaming.py` - src/ tests (10 tests)
- `tests/backend/local/core/test_streaming_llm_tts.py` - backend/ tests (13 tests)
- `tests/run_all_tests.py` - Unified runner

### Documentation:
- `COMPLETE_IMPLEMENTATION_SUMMARY.md` - **START HERE**
- `TESTING_GUIDE.md` - Testing
- `SRC_STREAMING_GUIDE.md` - src/ usage
- `TTS_STREAMING_STATUS.md` - TTS analysis
- `STREAMING_ISSUES_EXPLAINED.md` - Known issues
- `TTS_SSL_FIX_GUIDE.md` - Troubleshooting
- `GLM_MODEL_FIX.md` - Model config

---

## âš™ï¸ Configuration

### GLM Model
```python
model = "GLM-4-Flash"  # âœ… Correct
api_base = "https://open.bigmodel.cn/api/paas/v4/"
```

### TTS Settings
```python
chunk_size = 4096       # Audio chunk size
voice = "en-US-JennyNeural"  # Voice
rate = "+0%"            # Speech rate
```

### Streaming
```python
sentence_endings = [".", "!", "?", "\n"]
buffer_threshold = 100  # Characters before TTS
```

---

## ğŸ§ª Test Commands

```bash
# All tests
uv run python tests/run_all_tests.py

# Quick tests only
uv run python tests/run_all_tests.py --quick

# src/ tests only
uv run python tests/run_all_tests.py --src-only

# backend/ tests only
uv run python tests/run_all_tests.py --backend-only

# With HTML report
uv run python tests/run_all_tests.py --html

# Specific test
uv run python -m pytest tests/src/test_streaming.py::TestTTSStreaming -v
```

---

## ğŸ› Common Issues

### TTS SSL Error
```
SSLCertVerificationError: certificate has expired
```
**Fix**: `uv pip install --upgrade certifi edge-tts`

### GLM Model Error (1211)
```
æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ä»£ç 
```
**Fix**: Use `GLM-4-Flash` (not `glm-4-flashx`)

### Audio Buffer Overflow
```
ERROR: Audio buffer overflow
```
**Status**: Normal (occasional), only investigate if frequent

### Import Error
```
ModuleNotFoundError: No module named 'src'
```
**Fix**: Run from project root: `cd News_agent/`

---

## ğŸ“Š Performance

### Time-to-First-Audio

**Before**: 3300ms (sequential)
**After (backend/)**: 700ms (79% faster) âš¡
**After (src/)**: 2950ms (better UX)

---

## âœ… Test Status

| Category | Count | Status |
|----------|-------|--------|
| src/ Tests | 10 | âœ… |
| backend/ Tests | 13 | âœ… |
| VAD Tests | 5+ | âœ… |
| **Total** | **30+** | **âœ…** |

---

## ğŸ¯ Quick Usage

### src/ Desktop App
```python
from src.agent import NewsAgent
from src.voice_output import stream_tts_audio
import asyncio

agent = NewsAgent()

# LLM streaming
async for chunk in agent.get_response_stream("Hello"):
    print(chunk, end='')

# TTS streaming
async for audio in stream_tts_audio("Hello"):
    # Process 4KB chunk
    pass
```

### backend/ WebSocket
```python
from backend.app.core.streaming_handler import StreamingVoiceHandler

handler = StreamingVoiceHandler()

async for chunk in handler.process_voice_command_streaming(
    session_id, audio_bytes, "webm"
):
    print(f"{chunk['type']}: {chunk.get('text', chunk.get('data', ''))}")
```

---

## ğŸ“– Learn More

**Full details**: [COMPLETE_IMPLEMENTATION_SUMMARY.md](COMPLETE_IMPLEMENTATION_SUMMARY.md)

**Need help?**: Check the documentation guides above!
